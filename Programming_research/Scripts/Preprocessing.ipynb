{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to analyse and preprocess the data from the WikiSQL dataset.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: basic python preprocessing packages and json parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start with the usual cells for utility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages used for processing: \n",
    "import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "\n",
    "# for json parsing:\n",
    "import json\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "Models\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data/WikiSQL/data\" # the data path\n",
    "\n",
    "train_files = {\n",
    "    \"questions\": os.path.join(data_path, \"train.jsonl\"),\n",
    "    \"tables\": os.path.join(data_path, \"train.tables.jsonl\")\n",
    "}\n",
    "\n",
    "base_model_path = '../Models'\n",
    "\n",
    "processed_data_file_path = os.path.join(data_path, \"processed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.db\n",
      "dev.jsonl\n",
      "dev.tables.jsonl\n",
      "test.db\n",
      "test.jsonl\n",
      "test.tables.jsonl\n",
      "train.db\n",
      "train.jsonl\n",
      "train.tables.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents of the data path\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The relevant files are in jsonl format. Let's load the train.jsonl file and check out its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate a list of json objects as python dictionaries.\n",
    "def preliminary_parsing(files): \n",
    "    '''\n",
    "        function to extract the preliminary data from the json files and represent them in \n",
    "        python dictionary format\n",
    "        @param\n",
    "        files: a dict object that has the questions and tables files\n",
    "    '''\n",
    "    \n",
    "    # extract the two files from the input parameter\n",
    "    (questions, tables) = (files[\"questions\"], files[\"tables\"])\n",
    "    \n",
    "    # first generate the questions data:\n",
    "    qdata = list() # initialize to empty list\n",
    "    with open(questions, 'r') as ques:\n",
    "        for line in ques:\n",
    "            qdata.append(json.loads(line))\n",
    "            \n",
    "    # now generate the tables data\n",
    "    # the code for doing this quite same as before\n",
    "    tabdata = list() # initialize to empty list\n",
    "    with open(tables, 'r') as tabs:\n",
    "        for line in tabs:\n",
    "            tabdata.append(json.loads(line))\n",
    "\n",
    "    # transform the tabdata into a format easy for processing\n",
    "    modified_tabdata = {}\n",
    "    for item in tabdata:\n",
    "        key = item['id']\n",
    "        del item['id'] # delete the id from the table object\n",
    "        modified_tabdata[key] = item\n",
    "            \n",
    "    # return the extracted data:\n",
    "    return qdata, modified_tabdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61297"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdata, tabdata = preliminary_parsing(train_files)\n",
    "len(qdata) # print the no. of examples to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'phase': 2,\n",
       " u'question': u'How many bronzes were won for the country that had a larger than 3 rank and a silver win count above 0?',\n",
       " u'sql': {u'agg': 4, u'conds': [[0, 1, 3], [3, 1, 0]], u'sel': 4},\n",
       " u'table_id': u'2-1767441-2'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a sample example to train on.\n",
    "qdata[np.random.randint(len(qdata))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the above cell a few number of times to get a feeling of what the data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a query from the given python dictionary\n",
    "def generate_query(que, table_data):\n",
    "    '''\n",
    "        This returns a query string generated from the python dictionary\n",
    "        @param\n",
    "        que => the query in python dictionary format\n",
    "        table_data => the data of the tables (from the dataset itself)\n",
    "        \n",
    "        @return\n",
    "        Query string for the given query\n",
    "    '''\n",
    "    \n",
    "    # extract the building parameters of the query\n",
    "    params = que[\"sql\"]\n",
    "    \n",
    "    # The aggregation and condition ops in the format taken from the query dependency in lib:\n",
    "    agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "    cond_ops = ['=', '>', '<', 'OP']\n",
    "    \n",
    "    # initialize query\n",
    "    query = \"\"\n",
    "    \n",
    "    # generate select part of the query:\n",
    "    agg_operation = params[\"agg\"]\n",
    "    table_columns = table_data[que[\"table_id\"]][\"header\"]\n",
    "    select_column = table_columns[params[\"sel\"]]\n",
    "    \n",
    "    if(agg_operation != 0):\n",
    "        query += \"SELECT \" + agg_ops[agg_operation] + ' ( ' + select_column + ' ) '\n",
    "    else:\n",
    "        query += \"SELECT \" + select_column\n",
    "    \n",
    "    # keep the table name generic since they will only add randomness to the learning\n",
    "    query += \" FROM <TABLE>\"\n",
    "    \n",
    "    # add the where and conditions clause so on:\n",
    "    conditions = []\n",
    "    for cond in params['conds']:\n",
    "        col_index = cond[0]; cond_op = cond[1]; cond_target = cond[2]\n",
    "        \n",
    "        # add a condition\n",
    "        conditions.append(table_columns[col_index] + \" \" \n",
    "                          + cond_ops[cond_op] + \" \" + unicode(cond_target))\n",
    "    if(len(conditions) != 0):\n",
    "        # conditions not empty:\n",
    "        condition_clause = reduce(lambda x,y: x + \" AND \" + y, conditions)\n",
    "\n",
    "        query += \" WHERE \" + condition_clause\n",
    "        \n",
    "    # return the query so formed\n",
    "    return query\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original format: {u'phase': 1, u'table_id': u'1-10128185-2', u'question': u'How many votes did Northern Ireland cast if the total was 35?', u'sql': {u'agg': 0, u'sel': 2, u'conds': [[7, 0, 35]]}}\n",
      "\n",
      "\n",
      "Generated query: SELECT Northern Ireland FROM <TABLE> WHERE Total = 35\n"
     ]
    }
   ],
   "source": [
    "random_query = qdata[np.random.randint(len(qdata))]\n",
    "print \"Original format: \" + str(random_query) + \"\\n\\n\"\n",
    "print \"Generated query: \" + generate_query(random_query, tabdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the lists of input and ideal_output from the given dataset by using the so defined generate_query \n",
    "# function\n",
    "\n",
    "questions = np.array([elem[\"question\"] for elem in qdata])\n",
    "queries = np.array([generate_query(elem, tabdata) for elem in qdata])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic preprocessing of the data has been completed. Run the below cell a few times to make sure that the data has been properly processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample from the dataset:\n",
      "\n",
      "\n",
      "Natural_Language question: Who was the top goalscorer for season 2001-02?\n",
      "SQL query for the same   : SELECT Top Goalscorer FROM <TABLE> WHERE Season = 2001-02\n"
     ]
    }
   ],
   "source": [
    "# now: check the ideal output for a random question from the dataset\n",
    "random_index = np.random.randint(questions.shape[0])\n",
    "\n",
    "print \"Random sample from the dataset:\\n\\n\"\n",
    "\n",
    "# print the natural language question:\n",
    "print \"Natural_Language question: \" + questions[random_index]\n",
    "print \"SQL query for the same   : \" + queries[random_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now pickle this processed data to be used later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_data = {\n",
    "    \"questions\": questions,\n",
    "    \"queries\": queries\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(processed_data_file_path, \"w\") as dumping:\n",
    "    pickle.dump(processed_data, dumping, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data has been processed, now this can be used further to generate embeddings and later on for training the network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
