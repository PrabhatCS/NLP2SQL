{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Script to create a seq2seq model for training on the given data\n",
    "-----------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: Tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start with the usual cells for utility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for processing: \n",
    "import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# the boss of frameworks\n",
    "import tensorflow as tf\n",
    "\n",
    "# for dataset building:\n",
    "import collections\n",
    "\n",
    "# for regex based preprocessing\n",
    "import re\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "Models\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data/WikiSQL/data\" # the data path\n",
    "\n",
    "train_files = {\n",
    "    \"questions\": os.path.join(data_path, \"train.jsonl\"),\n",
    "    \"tables\": os.path.join(data_path, \"train.tables.jsonl\")\n",
    "}\n",
    "\n",
    "base_model_path = '../Models'\n",
    "\n",
    "processed_data_file_path = os.path.join(data_path, \"processed.pickle\")\n",
    "plug_and_play_data_file_path = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "# constants:\n",
    "matcher_regex = r\"[\\w']+|[.,!?;\\\"]\"\n",
    "vocab_size = 55000 # total words in our vocabulary\n",
    "lstm_hidden_state_size = 512 # hidden state size\n",
    "seqs_length = 85\n",
    "no_of_epochs = 5000\n",
    "batch_size = 128 # we look at only 64 examples in a single batch            \n",
    "checkpoint_factor = 50 # save the model after every 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.db\n",
      "dev.jsonl\n",
      "dev.tables.jsonl\n",
      "plug_and_play.pickle\n",
      "processed.pickle\n",
      "test.db\n",
      "test.jsonl\n",
      "test.tables.jsonl\n",
      "train.db\n",
      "train.jsonl\n",
      "train.tables.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents of the data path\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function to unpickle the data into a python object\n",
    "def unpickle(pickle_file):\n",
    "    '''\n",
    "        function to unpickle the pickle file into a python compatible object\n",
    "        @param\n",
    "        pickle => the pickle file path\n",
    "        @return => the unpickled object\n",
    "    '''\n",
    "    with open(pickle_file) as dumper:\n",
    "        return pickle.load(dumper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the processed pickled data into the script.\n",
    "data = unpickle(processed_data_file_path)\n",
    "\n",
    "queries = data[\"queries\"]\n",
    "questions = data[\"questions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61297, 61297)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions), len(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since all are questions, we can safely drop the question mark at the end of the question sentences\n",
    "### Another reason for doing this is that some examples have a question mark while others don't\n",
    "### Besides, there doesn't seem to be anything unique that can be learnt by adding it to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop question mark from end of all the questions if there exists one\n",
    "for index in range(len(questions)):\n",
    "    orig = questions[index]\n",
    "    orig_ans = queries[index]\n",
    "    \n",
    "    # remove the question mark if it exists\n",
    "    if(orig[-1] == '?'):\n",
    "        # remove the question mark from the end of the sentence.\n",
    "        orig = orig[:-1]\n",
    "        \n",
    "    # make everything lowercase:\n",
    "    questions[index] = orig.lower()\n",
    "    queries[index] = orig_ans.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample from the dataset:\n",
      "\n",
      "Natural_Language question: what grade was the 2.4km run (min:sec) of 13:01-13:40\n",
      "SQL query for the same   : select grade from <table> where 2.4km run (min:sec) = 13:01-13:40\n",
      "\n",
      "\n",
      "Random sample from the dataset:\n",
      "\n",
      "Natural_Language question: what is karen handel polling at in the insideradvantage poll where john oxendine is at 15%\n",
      "SQL query for the same   : select karen handel from <table> where poll source = insideradvantage and john oxendine = 15%\n",
      "\n",
      "\n",
      "Random sample from the dataset:\n",
      "\n",
      "Natural_Language question: name the country that has ken doherty\n",
      "SQL query for the same   : select country from <table> where athlete = ken doherty\n",
      "\n",
      "\n",
      "Random sample from the dataset:\n",
      "\n",
      "Natural_Language question: what is the adjusted gdp when the nominal gdp per capita is 2874\n",
      "SQL query for the same   : select gdp adjusted ($ billions) from <table> where gdp per capita nominal ($) = 2874\n",
      "\n",
      "\n",
      "Random sample from the dataset:\n",
      "\n",
      "Natural_Language question: which surface has a tournament of alphen aan den rijn, netherlands\n",
      "SQL query for the same   : select surface from <table> where tournament = alphen aan den rijn , netherlands\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    # now: check the ideal output for a random question from the dataset\n",
    "    random_index = np.random.randint(questions.shape[0])\n",
    "\n",
    "    print \"Random sample from the dataset:\\n\"\n",
    "\n",
    "    # print the natural language question:\n",
    "    print \"Natural_Language question: \" + questions[random_index]\n",
    "    print \"SQL query for the same   : \" + queries[random_index] + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(questions), type(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to build the dataset for the given task:\n",
    "\n",
    "def build_dataset(words, questions, queries, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['<blank>', 0], ['<go>', 1], ['<eos>', 2], ['UNK', -1]] # start with this list.\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1)) # this is inplace. i.e. has a side effect\n",
    "\n",
    "    dictionary = dict() # initialize the dictionary to empty one\n",
    "    # fill this dictionary with the most frequent words\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "  \n",
    "    # loop to replace all the rare words by the UNK token\n",
    "    data_questions = list() # start with empty list\n",
    "    data_queries = list()\n",
    "    unk_count = 0 # counter for keeping track of the unknown words\n",
    "    for question, query in zip(questions, queries):\n",
    "        \n",
    "        # first transform the question\n",
    "        data = [] # initilalize to empty\n",
    "        for word in question:\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        data_questions.append(data)\n",
    "        \n",
    "        # now transform the query\n",
    "        data = [] # initilalize to empty\n",
    "        for word in query:\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        data_queries.append(data)\n",
    "\n",
    "    count[0][1] = unk_count # replace the earlier -1 by the so calculated unknown count\n",
    "\n",
    "    print(\"Total rare words replaced: \", unk_count) # log the total replaced rare words\n",
    "  \n",
    "    # construct the reverse dictionary for the original dictionary\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    # return all the relevant stuff\t\n",
    "    return data_questions, data_queries, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the sentences into words\n",
    "split_questions = map(lambda x: x.split(), questions)\n",
    "split_queries = map(lambda x: x.split(), queries)\n",
    "\n",
    "#small loop to put all the words together:\n",
    "all_splits = map(lambda x: x.split(), list(questions) + list(queries))\n",
    "all_words = []\n",
    "for split_sentence in all_splits:\n",
    "    all_words += split_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words in the dataset   : 1473073\n",
      "unique words in the dataset: 55713\n"
     ]
    }
   ],
   "source": [
    "print \"All words in the dataset   : \" + str(len(all_words))\n",
    "print \"unique words in the dataset: \" + str(len(list(set(all_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total rare words replaced: ', 714)\n"
     ]
    }
   ],
   "source": [
    "dquestions, dqueries, count, dictionary, reverse_dictionary = build_dataset(all_words, \n",
    "                                                                        split_questions, split_queries, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[184, 178, 11, 5, 281, 79, 19, 189, 424], [11, 12, 5, 441, 101, 7, 5, 161, 101, 1582, 23, 173, 155], [11, 12, 5, 265, 19, 189, 424]], [[8, 281, 6, 9, 7, 441, 53789, 4, 189, 424], [8, 441, 101, 6, 9, 7, 281, 4, 161, 101, 1582, 23, 173, 155], [8, 265, 6, 9, 7, 4677, 4, 189, 424]])\n"
     ]
    }
   ],
   "source": [
    "print (dquestions[:3], dqueries[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add go and eos entry to every sequence. and then pad the sequence to the fixed length\n",
    "index = 0\n",
    "for (dquestion, dquery) in zip(dquestions, dqueries):\n",
    "    dquery = [1] + dquery + [2]\n",
    "    \n",
    "    # pad the dquestion\n",
    "    while(len(dquestion) != seqs_length):\n",
    "        dquestion += [0]\n",
    "    \n",
    "    # pad the dquery:\n",
    "    while(len(dquery) != seqs_length):\n",
    "        dquery += [0]\n",
    "    \n",
    "    dqueries[index] = dquery\n",
    "    dquestions[index] = dquestion\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 85)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: len(x), dquestions)) / len(dquestions), sum(map(lambda x: len(x), dqueries)) / len(dqueries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'tell', u'me', u'what', u'the', u'notes', u'are', u'for', u'south', u'australia', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>']\n"
     ]
    }
   ],
   "source": [
    "# again visualize a few sequences:\n",
    "print [reverse_dictionary[i] for i in dquestions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been pickled\n"
     ]
    }
   ],
   "source": [
    "# pickle the proper data so that we can use it in a plug and play manner\n",
    "plug_and_play_data = {\n",
    "    \"dictionary\": dictionary,\n",
    "    \"reverse_dictionary\": reverse_dictionary,\n",
    "    \"questions\": dquestions,\n",
    "    \"queries\": dqueries\n",
    "}\n",
    "\n",
    "if(not os.path.isfile(plug_and_play_data_file_path)):\n",
    "    with open(plug_and_play_data_file_path, 'wb') as dumping:\n",
    "        pickle.dump(plug_and_play_data, dumping, pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    print \"The data has been pickled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, now the data is processsed and ready for LSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation graph defining the network architecture of the model\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # constant to hold the training data and labels\n",
    "    with tf.variable_scope(\"input\"):\n",
    "        input_data = [tf.placeholder(tf.int32, shape=(seqs_length), name=\"input_sequences\")] # list of placeholders\n",
    "        input_translation = [tf.placeholder(tf.int32, shape=(seqs_length), name=\"ideal_output_sequences\")]\n",
    "        \n",
    "        loss_targets = tf.one_hot(tf.stack(input_translation), depth=len(dictionary.keys()))\n",
    "\n",
    "    # create the basic_rnn_seq2seq\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq (\n",
    "                        input_data, # encoder input\n",
    "                        input_translation, # decoder input\n",
    "                        tf.contrib.rnn.LSTMCell(lstm_hidden_state_size),\n",
    "                        len(dictionary.keys()),\n",
    "                        len(dictionary.keys()),\n",
    "                        128,\n",
    "                        feed_previous = True\n",
    "                      )\n",
    "    \n",
    "    # The output is now required for calculating the loss.\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        # we use the euclidean loss as the measure of minimization\n",
    "        # loss = tf.reduce_mean(tf.abs(tf.stack(outputs) - tf.stack(input_translation)), name=\"mean_loss\")\n",
    "        \n",
    "        # changed the loss to softmax_cross_entropy_with_logits\n",
    "        loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels = loss_targets,\n",
    "                logits = outputs\n",
    "            ))\n",
    "        loss_summary = tf.summary.scalar(\"loss_summary\", loss)\n",
    "        \n",
    "    with tf.variable_scope(\"prediction\"):\n",
    "        # define the op to calculate the predictions\n",
    "        prediction = tf.argmax(tf.stack(outputs), axis = -1)\n",
    "        \n",
    "    \n",
    "    # the train module for running the optimization op\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    \n",
    "    all_summaries = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'prediction/ArgMax:0' shape=(1, 85) dtype=int64>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"Model3\"\n",
    "itera = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_index = 0\n",
    "data_size = len(dquestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to decode the encoded query:\n",
    "def decode(encoded_list):\n",
    "    '''\n",
    "        function to decode the integer sequence\n",
    "        @param\n",
    "        encoded_list => the sequence of integers to decode\n",
    "        @return => the string fromed by decoding the input sequence\n",
    "    '''\n",
    "    \n",
    "    # generate the decoded words from the given list:\n",
    "    decoded_list = [] # start with empty list\n",
    "    for word in encoded_list:\n",
    "        if(word in reverse_dictionary):\n",
    "            decoded_list.append(reverse_dictionary[word])\n",
    "        else:\n",
    "            decoded_list.append('UNK')\n",
    "    \n",
    "    decoded_string = reduce(lambda x, y: x + \" \" + y, decoded_list)\n",
    "    \n",
    "    # return the decoded string\n",
    "    return decoded_string.replace(\"<blank>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time to run this session:\n",
    "'''\n",
    "code snippet to run a tensorflow session for performing the training.\n",
    "'''\n",
    "\n",
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "'''\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # bring the global_index into current scope:\n",
    "    global global_index\n",
    "    \n",
    "    # The saver object for saving and loading the model\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    # the path where the model will be saved \n",
    "    # let's visualize this graph in tensorboard:\n",
    "    model_path = os.path.join(base_model_path, model_name)\n",
    "    \n",
    "    # create the summary_writer for tensorboard\n",
    "    tensorboard_writer = tf.summary.FileWriter(model_path, graph=sess.graph)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the weights from the model\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # initialize all the variables\n",
    "        sess.run(init)\n",
    "    \n",
    "    for ep in range((itera - 1) * no_of_epochs, itera * no_of_epochs):  # start the loop \n",
    "        \n",
    "        start = global_index\n",
    "        end = start + batch_size\n",
    "            \n",
    "        questions_raw = dquestions[start: end]\n",
    "        queries_raw = dqueries[start: end]\n",
    "            \n",
    "        input_questions_batch = list(np.array(questions_raw).reshape(len(questions_raw), seqs_length))\n",
    "        input_translate_batch = list(np.array(queries_raw).reshape(len(queries_raw), seqs_length))\n",
    "            \n",
    "        global_index = (global_index + batch_size) % data_size\n",
    "        \n",
    "        # construct the feed dictionary\n",
    "        ques_dict = {i: d for (i, d) in zip(input_data, input_questions_batch)}\n",
    "        quer_dict = {i: d for (i, d) in zip(input_translation, input_translate_batch)}\n",
    "        combined_dict = dict(ques_dict.items() + quer_dict.items())\n",
    "        \n",
    "        # execute the training op\n",
    "        _, cost = sess.run([train_op, loss], feed_dict=combined_dict)\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0):\n",
    "            # print the log statements:\n",
    "            print \"epoch: \" + str(ep + 1)\n",
    "            print \"=================================================================================================\"\n",
    "            print \"=================================================================================================\"\n",
    "            print('loss = {}'.format(cost))\n",
    "            # run a random prediction:\n",
    "            random_index = np.random.randint(batch_size)\n",
    "            ques = input_questions_batch[random_index]\n",
    "            ideal_quer = input_translate_batch[random_index]\n",
    "            mock = np.array([dictionary['<go>']] + [dictionary['blank'] for _ in range(seqs_length - 1)])\n",
    "            \n",
    "            quer = sess.run(prediction, feed_dict={input_data[0]: ques, input_translation[0]: mock})\n",
    "            \n",
    "            print \"Input Question  : \" + decode(ques)\n",
    "            print \"Ideal Output    : \" + decode(ideal_quer)\n",
    "            print \"Output Received : \" + decode(quer[0])\n",
    "            \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "            print \"=================================================================================================\"\n",
    "            print \"=================================================================================================\"\n",
    "            \n",
    "            # run the summary op also\n",
    "            summary = sess.run(all_summaries, feed_dict=combined_dict)\n",
    "\n",
    "            # add the generated summary to the fileWriter\n",
    "            tensorboard_writer.add_summary(summary, (ep + 1))\n",
    "            \n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, model_name), global_step = (ep + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training is now complete. Let's calculate the accuracy and also, try feeding in some random inputs to test how well the model works in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
