{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced dynamic seq2seq with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder is bidirectional now. Decoder is implemented using `tf.nn.raw_rnn`. \n",
    "It feeds previously generated tokens during training as inputs, instead of target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "regular seq2seq\n",
    "![seq2seq architecutre](pictures/1-seq2seq.jpg)\n",
    "Rectangles are encoder and decoder's recurrent layers. Encoder receives `[A, B, C]` sequence as inputs. We don't care about encoder outputs, only about the hidden state it accumulates while reading the sequence. After input sequence ends, encoder passes its final state to decoder, which receives `[<EOS>, W, X, Y, Z]` and is trained to output `[W, X, Y, Z, <EOS>]`. `<EOS>` token is a special word in vocabulary that signals to decoder the beginning of translation.\n",
    "\n",
    "## Implementation details\n",
    "\n",
    "TensorFlow has its own [implementation of seq2seq](https://www.tensorflow.org/tutorials/seq2seq/). Recently it was moved from core examples to [`tensorflow/models` repo](https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate), and uses deprecated seq2seq implementation. Deprecation happened because it uses **static unrolling**.\n",
    "\n",
    "**Static unrolling** involves construction of computation graph with a fixed sequence of time step. Such a graph can only handle sequences of specific lengths. One solution for handling sequences of varying lengths is to create multiple graphs with different time lengths and separate the dataset into this buckets.\n",
    "\n",
    "**Dynamic unrolling** instead uses control flow ops to process sequence step by step. In TF this is supposed to more space efficient and just as fast. This is now a recommended way to implement RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we give encoder input sequence like 'hello how are you', we take the last hidden state and feed to decoder and it\n",
    "#will generate a decoded value. we compare that to target value, if translation would be 'bonjour ca va' and minimize \n",
    "#the difference by optimizing a loss function\n",
    "\n",
    "#in this case we just want to encode and decode the input successfully\n",
    "\n",
    "#bidirectional encoder\n",
    "#We will teach our model to memorize and reproduce input sequence. \n",
    "#Sequences will be random, with varying length.\n",
    "#Since random sequences do not contain any structure, \n",
    "#model will not be able to exploit any patterns in data. \n",
    "#It will simply encode sequence in a thought vector, then decode from it.\n",
    "#this is not about prediction (end goal), it's about understanding this architecture\n",
    "\n",
    "#this is an encoder-decoder architecture. The encoder is bidrectional so \n",
    "#it It feeds previously generated tokens during training as inputs, instead of target sequence.\n",
    "\n",
    "import numpy as np #matrix math \n",
    "import tensorflow as tf #machine learningt\n",
    "import helpers #for formatting data into batches and generating random sequence data\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession() #initializes a tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First critical thing to decide: vocabulary size.\n",
    "#Dynamic RNN models can be adapted to different batch sizes \n",
    "#and sequence lengths without retraining \n",
    "#(e.g. by serializing model parameters and Graph definitions via tf.train.Saver), \n",
    "#but changing vocabulary size requires retraining the model.\n",
    "\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20 #character length\n",
    "\n",
    "encoder_hidden_units = 256 #num neurons\n",
    "decoder_hidden_units = encoder_hidden_units * 2 #in original paper, they used same number of neurons for both encoder\n",
    "#and decoder, but we use twice as many so decoded output is different, the target value is the original input \n",
    "#in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice way to understand complicated function is to study its signature - inputs and outputs. With pure functions, only inputs-output relation matters.\n",
    "\n",
    "- `encoder_inputs` int32 tensor is shaped `[encoder_max_time, batch_size]`\n",
    "- `decoder_targets` int32 tensor is shaped `[decoder_max_time, batch_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input placehodlers\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "# contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "# if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we implement decoder with `tf.nn.raw_rnn` and will construct `decoder_inputs` step by step in the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "`encoder_inputs` and `decoder_inputs` are int32 tensors of shape `[max_time, batch_size]`, while encoder and decoder RNNs expect dense vector representation of words, `[max_time, batch_size, input_embedding_size]`. We convert one to another by using *word embeddings*. Specifics of working with embeddings are nicely described in [official tutorial on embeddings](https://www.tensorflow.org/tutorials/word2vec/).\n",
    "\n",
    "First we initialize embedding matrix. Initializations are random. We rely on our end-to-end training to learn vector representations for words jointly with encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomly initialized embedding matrrix that can fit input sequence\n",
    "#used to convert sequences to vectors (embeddings) for both encoder and decoder of the right size\n",
    "#reshaping is a thing, in TF you gotta make sure you tensors are the right shape (num dimensions)\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "#this thing could get huge in a real world application\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'encoder_inputs:0' shape=(?, ?) dtype=int32>,\n",
       " <tf.Tensor 'embedding_lookup:0' shape=(?, ?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The embedding_lookup embeds the input integers.\n",
    "encoder_inputs, encoder_inputs_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `tf.nn.embedding_lookup` to *index embedding matrix*: given word `4`, we represent it as 4th column of embedding matrix. \n",
    "This operation is lightweight, compared with alternative approach of one-hot encoding word `4` as `[0,0,0,1,0,0,0,0,0,0]` (vocab size 10) and then multiplying it by embedding matrix.\n",
    "\n",
    "Additionally, we don't need to compute gradients for any columns except 4th.\n",
    "\n",
    "In real NLP application embedding matrix can get very large, with 100k or even 1m columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "We are replacing unidirectional `tf.nn.dynamic_rnn` with `tf.nn.bidirectional_dynamic_rnn` as the encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn_cell import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units, num_proj = input_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get outputs and states\n",
    "#bidirectional RNN function takes a separate cell argument for \n",
    "#both the forward and backward RNN, and returns separate \n",
    "#outputs and states for both the forward and backward RNN\n",
    "\n",
    "#When using a standard RNN to make predictions we are only taking the “past” into account. \n",
    "#For certain tasks this makes sense (e.g. predicting the next word), but for some tasks \n",
    "#it would be useful to take both the past and the future into account. Think of a tagging task, \n",
    "#like part-of-speech tagging, where we want to assign a tag to each word in a sentence. \n",
    "#Here we already know the full sequence of words, and for each word we want to take not only the \n",
    "#words to the left (past) but also the words to the right (future) into account when making a prediction. \n",
    "#Bidirectional RNNs do exactly that. A bidirectional RNN is a combination of two RNNs – one runs forward from \n",
    "#“left to right” and one runs backward from “right to left”. These are commonly used for tagging tasks, or \n",
    "#when we want to embed a sequence into a fixed-length vector (beyond the scope of this post).\n",
    "\n",
    "\n",
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 256) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 256) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to concatenate forward and backward outputs and state. In this case we will not discard outputs, they would be used for attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Concatenates tensors along one dimension.\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
    "\n",
    "encoder_final_state_c = tf.concat (\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat (\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "encoder_final_state = LSTMStateTuple (\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'concat_1:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'concat_2:0' shape=(?, 40) dtype=float32>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units, num_proj=(input_embedding_size * 2)) # no need to use num_proj here **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time and batch dimensions are dynamic, i.e. they can change in runtime, from batch to batch\n",
    " When decoding, feeding previously generated tokens as inputs adds robustness to model's errors. However feeding ground truth speeds up training. Apperantly best practice is to mix both randomly when training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we could print this, won't need\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'encoder_inputs:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to decide how far to run decoder. There are several options for stopping criteria:\n",
    "- Stop after specified number of unrolling steps\n",
    "- Stop after model produced <EOS> token\n",
    "\n",
    "The choice will likely be time-dependant. In legacy `translate` tutorial we can see that decoder unrolls for `len(encoder_input)+10` to allow for possibly longer translated sequence. Here we are doing a toy copy task, so how about we unroll decoder for `len(encoder_input)+2`, to allow model some room to make mistakes over 2 additional steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs (** Important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output projection\n",
    "\n",
    "Decoder will contain manually specified by us transition step:\n",
    "```\n",
    "output(t) -> output projection(t) -> prediction(t) (argmax) -> input embedding(t+1) -> input(t+1)\n",
    "```\n",
    "\n",
    "In tutorial 1, we used `tf.contrib.layers.linear` layer to initialize weights and biases and apply operation for us. This is convenient, however now we need to specify parameters `W` and `b`  of the output layer in global scope, and apply them at every step of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually specifying since we are going to implement attention details for the decoder in a sec\n",
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([input_embedding_size * 2, vocab_size], -1, 1), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable_1:0' shape=(40, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_2:0' shape=(10,) dtype=float32_ref>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder via `tf.nn.raw_rnn`\n",
    "\n",
    "`tf.nn.dynamic_rnn` allows for easy RNN construction, but is limited. \n",
    "\n",
    "For example, a nice way to increase robustness of the model is to feed as decoder inputs tokens that it previously generated, instead of shifted true sequence.\n",
    "\n",
    "![seq2seq-feed-previous](pictures/2-seq2seq-feed-previous.png)\n",
    "*Image borrowed from http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First prepare tokens. Decoder would operate on column vectors of shape `(batch_size,)` representing single time steps of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create padded inputs for the decoder from the word embeddings\n",
    "\n",
    "#were telling the program to test a condition, and trigger an error if the condition is false.\n",
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "#retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'EOS:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'PAD:0' shape=(?,) dtype=int32>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_time_slice, pad_time_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'embedding_lookup_1:0' shape=(?, 20) dtype=float32>,\n",
       " <tf.Tensor 'embedding_lookup_2:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_step_embedded, pad_step_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the tricky part.\n",
    "\n",
    "Remember that standard `tf.nn.dynamic_rnn` requires all inputs `(t, ..., t+n)` be passed in advance as a single tensor. \"Dynamic\" part of its name refers to the fact that `n` can change from batch to batch.\n",
    "\n",
    "Now, what if we want to implement more complex mechanic like when we want decoder to receive previously generated tokens as input at every timestamp (instead of lagged target sequence)? Or when we want to implement soft attention, where at every timestep we add additional fixed-len representation, derived from query produced by previous step's hidden state? `tf.nn.raw_rnn` is a way to solve this problem.\n",
    "\n",
    "Main part of specifying RNN with `tf.nn.raw_rnn` is *loop transition function*. It defines inputs of step `t` given outputs and state of step `t-1`.\n",
    "\n",
    "Loop transition function is a mapping `(time, previous_cell_output, previous_cell_state, previous_loop_state) -> (elements_finished, input, cell_state, output, loop_state)`. It is called *before* RNNCell to prepare its inputs and state. Everything is a Tensor except for initial call at time=0 when everything is `None` (except `time`).\n",
    "\n",
    "Note that decoder inputs are returned from the transition function but passed into it. You are supposed to index inputs manually using `time` Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop transition function is called two times:\n",
    " 1. Initial call at time=0 to provide initial cell_state and input to RNN.\n",
    " 2. Transition call for all following timesteps where you define transition between two adjacent steps.\n",
    "\n",
    "Lets define both cases separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop initial state is function of only `encoder_final_state` and embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually specifying loop function through time - to get initial cell state and input to RNN\n",
    "#normally we'd just use dynamic_rnn, but lets get detailed here with raw_rnn\n",
    "\n",
    "#we define and return these values, no operations occur here\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    #end of sentence\n",
    "    initial_input = eos_step_embedded\n",
    "    #last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "    #none\n",
    "    initial_cell_output = None\n",
    "    #none\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transition function such that previously generated token (as judged in greedy manner by `argmax` over output projection) is passed as next input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#attention mechanism --choose which previously generated token to pass as input in the next timestep\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    \n",
    "    def get_next_input():\n",
    "        #dot product between previous ouput and weights, then + biases\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        #Logits simply means that the function operates on the unscaled output of \n",
    "        #earlier layers and that the relative scale to understand the units is linear. \n",
    "        #It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
    "        #(you might have an input of 5).\n",
    "        #prediction value at current time step\n",
    "        \n",
    "        #Returns the index with the largest value across axes of a tensor.\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        #embed prediction for the next input\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    \n",
    "    \n",
    "    #Computes the \"logical and\" of elements across dimensions of a tensor.\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    #Return either fn1() or fn2() based on the boolean predicate pred.\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    \n",
    "    #set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine initializer and transition functions and create raw_rnn.\n",
    "\n",
    "Note that while all operations above are defined with TF's control flow and reduction ops, here we rely on checking if state is `None` to determine if it is an initializer call or transition call. This is not very clean API and might be changed in the future (indeed, `tf.nn.raw_rnn`'s doc contains warning that API is experimental)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
    "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
    "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
    "#and what to emit for the output.\n",
    "#ta = tensor array\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_5:0' shape=(?, 40) dtype=float32>),\n",
       " <tensorflow.python.ops.tensor_array_ops.TensorArray at 0x7f111c22b9d0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_final_state, decoder_outputs_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder_targets:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do output projection, we have to temporarilly flatten `decoder_outputs` from `[max_steps, batch_size, hidden_dim]` to `[max_steps*batch_size, hidden_dim]`, as `tf.matmul` needs rank-2 tensors at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to convert output to human readable prediction\n",
    "#we will reshape output tensor\n",
    "\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax_1:0' shape=(?, ?) dtype=int64>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder_targets:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_9:0' shape=(?, ?, 10) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN outputs tensor of shape `[max_time, batch_size, hidden_units]` which projection layer maps onto `[max_time, batch_size, vocab_size]`. `vocab_size` part of the shape is static, while `max_time` and `batch_size` is dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross entropy loss\n",
    "#one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the toy task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the copy task — given a random sequence of integers from a `vocabulary`, learn to memorize and reproduce input sequence. Because sequences are random, they do not contain any structure, unlike natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[7, 8, 3, 3]\n",
      "[2, 8, 8, 8, 3, 2]\n",
      "[2, 6, 7, 7, 3, 9, 2]\n",
      "[9, 7, 7, 8, 7]\n",
      "[2, 9, 2, 5]\n",
      "[7, 5, 9, 4, 2, 3, 9]\n",
      "[2, 5, 3, 3, 2]\n",
      "[4, 2, 6, 7, 8, 4, 6]\n",
      "[7, 5, 3, 2, 8, 4]\n",
      "[3, 8, 3, 8, 4]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.22708272934\n",
      "  sample 1:\n",
      "    input     > [6 8 6 3 4 6 0 0]\n",
      "    predicted > [3 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 6 3 8 5 6 8 0]\n",
      "    predicted > [5 5 5 7 9 9 1 3 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 5 2 0 0 0 0 0]\n",
      "    predicted > [2 2 2 2 0 0 0 0 0 0 0]\n",
      "()\n",
      "batch 1000\n",
      "  minibatch loss: 0.216761916876\n",
      "  sample 1:\n",
      "    input     > [5 2 7 7 0 0 0 0]\n",
      "    predicted > [5 2 7 7 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 2 4 9 0 0 0 0]\n",
      "    predicted > [6 2 4 9 1 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 4 7 2 4 0 0 0]\n",
      "    predicted > [7 4 7 2 4 1 0 0 0 0 0]\n",
      "()\n",
      "batch 2000\n",
      "  minibatch loss: 0.0884198695421\n",
      "  sample 1:\n",
      "    input     > [4 7 9 4 6 0 0 0]\n",
      "    predicted > [4 7 9 4 6 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 3 3 7 8 4 5 8]\n",
      "    predicted > [4 3 3 7 8 4 5 8 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 3 2 3 3 2 2 5]\n",
      "    predicted > [5 3 2 3 3 2 2 5 1 0 0]\n",
      "()\n",
      "batch 3000\n",
      "  minibatch loss: 0.0325066447258\n",
      "  sample 1:\n",
      "    input     > [4 6 8 0 0 0 0 0]\n",
      "    predicted > [4 6 8 1 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 3 8 0 0 0 0 0]\n",
      "    predicted > [5 3 8 1 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 6 2 3 3 7 2 7]\n",
      "    predicted > [4 6 2 3 3 7 2 7 1 0 0]\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0327 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f11146c7a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
